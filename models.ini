## This file contains the configuration for the models that are available in the system.
## The model names are the section names in this file. They are nicknames and not the official
## model names. To make it easier to select a model at runtime (e.g. haiku vs claude-3-haiku-20240307).
##
## Options listed here will roll-up and override the provider and default ones in config.ini, allowing you to
## have per-model settings. In addition, you can use the option "prompt = <filename>" to specify an model specific
## prompt file to use. This will override the default and fallback prompts. This can be useful if you want a
## particular mode to focus on code or have a different style of conversation better suited to it.

## You can extend and override this file by creating a file at ~/.config/iptic-memex/models.ini
[DEFAULT]
price_unit = 1000000
interactions = completion, chat

[gpt-5]
provider = OpenAI
model_name = gpt-5
context_size = 256000
price_in = 1.25
price_cache_in = 0.125
price_out = 10.00
reasoning = True
## reasoning_effort can be one of: minimal, low, medium, high
reasoning_effort = medium
## verbosity can be one of: low, medium, high
verbosity = low
## max_completion_tokens overrides max_tokens when 'reasoning' is True. If max_completion_tokens is omitted
## and max_tokens is set it will be mapped to max_completion_tokens
max_completion_tokens = 50000
response_label = "> GPT-5: "

[gpt-5-mini]
provider = OpenAI
model_name = gpt-5-mini
context_size = 256000
reasoning = true
reasoning_effort = minimal
verbosity = low
price_in = 0.25
price_cache_in = 0.03
price_out = 2.00
response_label = "> GPT-5-Mini: "

[gpt-5-resp]
provider = OpenAIResponses
model_name = gpt-5
# Enable streaming to exercise Responses event flow
stream = True
# Default to no storage unless user explicitly opts in via config
store = False
context_size = 256000
price_in = 1.25
price_cache_in = 0.125
price_out = 10.00
reasoning = True
## reasoning_effort can be one of: minimal, low, medium, high
reasoning_effort = medium
## verbosity can be one of: low, medium, high
verbosity = low
response_label = "> GPT-5 (Resp): "

[gpt-5-mini-resp]
provider = OpenAIResponses
model_name = gpt-5-mini
# Enable streaming to exercise Responses event flow
stream = True
# Default to no storage unless user explicitly opts in via config
store = False
context_size = 256000
reasoning = true
reasoning_effort = minimal
verbosity = low
price_in = 0.25
price_cache_in = 0.03
price_out = 2.00
response_label = "> GPT-5 (Resp): "

[gpt-4o]
provider = OpenAI
model_name = gpt-4o
context_size = 128000
price_in = 2.5
price_out = 10
vision = True

[gpt-4o-mini]
provider = OpenAI
model_name = gpt-4o-mini
context_size = 128000
price_in = 0.15
price_out = 0.60

[gpt-4.1]
provider = OpenAI
model_name = gpt-4.1
context_size = 1047576
max_tokens = 32768
price_in = 2.00
price_out = 8.00
price_cache_in = 0.50
vision = True

[gpt-4.1-mini]
provider = OpenAI
model_name = gpt-4.1
context_size = 1047576
max_tokens = 32768
price_in = 0.40
price_out = 1.60
price_cache_in = 0.10

[gpt-4.1-nano]
provider = OpenAI
model_name = gpt-4.1
context_size = 1047576
max_tokens = 32768
price_in = 0.10
price_out = 0.40
price_cache_in = 0.03

[o1]
provider = OpenAI
model_name = o1
context_size = 200000
response_label = "> o1: "
reasoning = True
price_in = 15.00
price_out = 60.00

[o1-mini]
provider = OpenAI
model_name = o1-mini
context_size = 128000
response_label = "> o1-mini: "
reasoning = True
price_in = 1.10
price_out = 4.40

[o3-mini]
provider = OpenAI
model_name = o3-mini
context_size = 200000
response_label = "> o3-mini: "
reasoning = True
price_in = 1.10
price_out = 4.40

[haiku]
provider = Anthropic
model_name = claude-3-5-haiku-latest
context_size = 200000
max_tokens = 4096
price_in = 0.8
price_out = 4
price_cache_in = 1
price_cache_out = 0.08
vision = True

[sonnet]
provider = Anthropic
model_name = claude-sonnet-4-0
context_size = 200000
price_in = 3
price_out = 15
price_cache_in = 3.75
price_cache_out = 0.03

[opus]
provider = Anthropic
model_name = claude-opus-4-0
context_size = 200000
price_in = 15
price_out = 75
price_cache_in = 18.75
price_cache_out = 1.5

[gemini-pro]
provider = Google
model_name = gemini-1.5-pro
context_size = 2097152
max_tokens = 8192
price_in = 1.25
price_out = 5
price_cache_in = 0.3125

[gemini-flash]
provider = Google
model_name = gemini-2.0-flash-001
context_size = 1048576
max_tokens = 8192
price_in = 0.10
price_out = 0.40
vision = True

[sonar]
provider = Perplexity
model_name = sonar
context_size = 127000
price_in = 1
price_out = 1

[sonar-pro]
provider = Perplexity
model_name = sonar-pro
context_size = 200000
price_in = 3
price_out = 15

[sonar-reasoning]
provider = Perplexity
model_name = sonar-reasoning
context_size = 127000
price_in = 1
price_out = 5

[sonar-reasoning-pro]
provider = Perplexity
model_name = sonar-reasoning-pro
context_size = 127000
price_in = 2
price_out = 8

[sonar-deep-research]
provider = Perplexity
model_name = sonar-deep-research
context_size = 60000
price_in = 2
price_out = 8

[deepseek-chat]
provider = DeepSeek
model_name = deepseek-chat
context_size = 131072
price_in = 0.14
price_out = 0.28

[deepseek-coder]
provider = DeepSeek
model_name = deepseek-coder
context_size = 131072
price_in = 0.14
price_out = 0.28

[deepseek-reasoner]
provider = DeepSeek
model_name = deepseek-reasoner
context_size = 131072
price_in = 0.14
price_out = 0.28

#[nemotron]
#provider = OpenRouter
#model_name = nvidia/nemotron-4-340b-instruct
#context_size = 4096
#response_label = "> Nemotron: "

# example of a model entry for use with llama.cpp, might be best to put these in the user's own models.ini file
#
#[llama3]
#provider = LlamaCpp
#model_name = llama3-70B
#model_path = <full path to model>
#context_size = 12000
#response_label = "> Llama: "
#extra_body = {cache_prompt:true}
#speculative = True # enable speculative decoding
#draft = 5 # number of tokens to draft, defaults to 10

[Mock]
provider = Mock
default = True
temperature = 0.7
max_tokens = 1000
