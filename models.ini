## This file contains the configuration for the models that are available in the system.
## The model names are the section names in this file. They are nicknames and not the official
## model names. To make it easier to select a model at runtime (e.g. haiku vs claude-3-haiku-20240307).
##
## Options listed here will roll-up and override the provider and default ones in config.ini, allowing you to
## have per-model settings. In addition, you can use the option "prompt = <filename>" to specify an model specific
## prompt file to use. This will override the default and fallback prompts. This can be useful if you want a
## particular mode to focus on code or have a different style of conversation better suited to it.

## You can extend and override this file by creating a file at ~/.config/iptic-memex/models.ini
[DEFAULT]
price_unit = 1000000
interactions = completion, chat

[gpt-4o-mini]
provider = OpenAI
model_name = gpt-4o-mini
context_size = 128000
price_in = 0.15
price_out = 0.60

[gpt-4-turbo]
provider = OpenAI
model_name = gpt-4-turbo
context_size = 128000
price_in = 10
price_out = 30

[gpt-4o]
provider = OpenAI
model_name = gpt-4o
context_size = 128000
price_in = 2.5
price_out = 10
vision = True

[gpt-4]
provider = OpenAI
model_name = gpt-4
context_size = 8192
price_in = 30
price_out = 60

[o1]
provider = OpenAI
model_name = o1
context_size = 200000
response_label = "> o1: "
reasoning = True
price_in = 15.00
price_out = 60.00

[o1-mini]
provider = OpenAI
model_name = o1-mini
context_size = 128000
response_label = "> o1-mini: "
reasoning = True
price_in = 1.10
price_out = 4.40

[o3-mini]
provider = OpenAI
model_name = o3-mini
context_size = 200000
response_label = "> o3-mini: "
reasoning = True
price_in = 1.10
price_out = 4.40

[haiku]
provider = Anthropic
model_name = claude-3-haiku-20240307
context_size = 200000
price_in = 0.25
price_out = 1.25
max_tokens = 4096
vision = True

[sonnet]
provider = Anthropic
model_name = claude-3-sonnet-20240229
context_size = 200000
price_in = 3
price_out = 15

[sonnet-3.5]
provider = Anthropic
model_name = claude-3-5-sonnet-latest
context_size = 200000
price_in = 3
price_out = 15

[opus]
provider = Anthropic
model_name = claude-3-opus-latest
context_size = 200000
price_in = 15
price_out = 75

[gemini-pro]
provider = Google
model_name = gemini-1.5-pro
context_size = 1000000
price_in = 3.5
price_out = 10.5

[gemini-flash]
provider = Google
model_name = gemini-1.5-flash
context_size = 1000000
price_in = 0.35
price_out = 1.05
vision = True

[gemini-pro-1.0]
provider = Google
model_name = gemini-1.0-pro
context_size = 1000000
price_in = 0.5
price_out = 1.5

[sonar]
provider = Perplexity
model_name = sonar
context_size = 127000
price_in = 1
price_out = 1

[sonar-pro]
provider = Perplexity
model_name = sonar-pro
context_size = 200000
price_in = 3
price_out = 15

[sonar-reasoning]
provider = Perplexity
model_name = sonar-reasoning
context_size = 127000
price_in = 1
price_out = 5

[groq-llama-8b]
provider = Groq
model_name = llama3-8b-8192
context_size = 8192
price_in = 0.00
price_out = 0.00

[groq-llama-70b]
provider = Groq
model_name = llama3-70b-8192
context_size = 8192
price_in = 0.00
price_out = 0.00

[codestral]
provider = Codestral
model_name = codestral-latest
context_size = 32768
price_in = 0.00
price_out = 0.00

[mistral-7b]
provider = Mistral
model_name = open-mistral-7b
context_size = 32768
price_in = 0.00
price_out = 0.00

[mixtral-8x7b]
provider = Mistral
model_name = open-mixtral-8x7b
context_size = 32768
price_in = 0.00
price_out = 0.00

[mixtral-8x22b]
provider = Mistral
model_name = open-mixtral-8x22b
context_size = 65536
price_in = 0.00
price_out = 0.00

[mistral-nemo]
provider = Mistral
model_name = open-mistral-nemo
context_size = 131072
price_in = 0.00
price_out = 0.00

[deepseek-chat]
provider = DeepSeek
model_name = deepseek-chat
context_size = 131072
price_in = 0.14
price_out = 0.28

[deepseek-coder]
provider = DeepSeek
model_name = deepseek-coder
context_size = 131072
price_in = 0.14
price_out = 0.28

[deepseek-reasoner]
provider = DeepSeek
model_name = deepseek-reasoner
context_size = 131072
price_in = 0.14
price_out = 0.28

[command-r]
provider = Cohere
model_name = command-r
context_size = 131072
price_in = 0.50
price_out = 1.50

[command-r-plus]
provider = Cohere
model_name = command-r-plus
context_size = 131072
price_in = 3.00
price_out = 15.00

[fw-llama3.1-405b]
provider = Fireworks
model_name = accounts/fireworks/models/llama-v3p1-405b-instruct
context_size = 131072
price_in = 3.00
price_out = 3.00

[tg-llama3.1-405b]
provider = Together
model_name = meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo
context_size = 4096
price_in = 5.00
price_out = 5.00

#[nemotron]
#provider = OpenRouter
#model_name = nvidia/nemotron-4-340b-instruct
#context_size = 4096
#response_label = "> Nemotron: "

# example of a model entry for use with llama.cpp, might be best to put these in the user's own models.ini file
#
#[llama3]
#provider = LlamaCpp
#model_name = llama3-70B
#model_path = <full path to model>
#context_size = 12000
#response_label = "> Llama: "
#extra_body = {cache_prompt:true}
#speculative = True # enable speculative decoding
#draft = 5 # number of tokens to draft, defaults to 10
