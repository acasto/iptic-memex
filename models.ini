## This file contains the configuration for the models that are available in the system.
## The model names are the section names in this file. They are nicknames and not the official
## model names. To make it easier to select a model at runtime (e.g. haiku vs claude-3-haiku-20240307).
##
## Options listed here will roll-up and override the provider and default ones in config.ini, allowing you to
## have per-model settings. In addition, you can use the option "prompt = <filename>" to specify an model specific
## prompt file to use. This will override the default and fallback prompts.

## You can extend and override this file by creating a file at ~/.config/iptic-memex/models.ini
[DEFAULT]
price_unit = 1000000
interactions = completion, chat

[gpt-3.5-turbo]
provider = OpenAI
model_name = gpt-3.5-turbo
context_size = 16384
price_in = 0.5
price_out = 1.5

[gpt-4-turbo]
provider = OpenAI
model_name = gpt-4-turbo
context_size = 128000
price_in = 10
price_out = 30

[gpt-4o]
provider = OpenAI
model_name = gpt-4o
context_size = 128000
price_in = 5
price_out = 15

[gpt-4]
provider = OpenAI
model_name = gpt-4
context_size = 8192
price_in = 30
price_out = 60

[haiku]
provider = Anthropic
model_name = claude-3-haiku-20240307
context_size = 200000
price_in = 0.25
price_out = 1.25

[sonnet-3]
provider = Anthropic
model_name = claude-3-sonnet-20240229
context_size = 200000
price_in = 3
price_out = 15

[sonnet]
provider = Anthropic
model_name = claude-3-5-sonnet-20240620
context_size = 200000
price_in = 3
price_out = 15

[opus]
provider = Anthropic
model_name = claude-3-opus-20240229
context_size = 200000
price_in = 15
price_out = 75

[gemini-pro]
provider = Google
model_name = gemini-1.5-pro
context_size = 1000000
price_in = 3.5
price_out = 10.5

[gemini-flash]
provider = Google
model_name = gemini-1.5-flash
context_size = 1000000
price_in = 0.35
price_out = 1.05

[gemini-pro-1.0]
provider = Google
model_name = gemini-1.0-pro
context_size = 1000000
price_in = 0.5
price_out = 1.5

# example of a model entry for use with llama.cpp, might be best to put these in the user's own models.ini file
#
#[llama3]
#provider = LlamaCpp
#model_name = llama3-70B
#context_size = 12000
#response_label = "> Llama: "
#extra_body = {cache_prompt:true}
