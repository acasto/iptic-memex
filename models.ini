## This file contains the configuration for the models that are available in the system.
## The model names are the section names in this file. They are nicknames and not the official
## model names. To make it easier to select a model at runtime (e.g. haiku vs claude-3-haiku-20240307).
##
## Options listed here will roll-up and override the provider and default ones in config.ini, allowing you to
## have per-model settings. In addition, you can use the option "prompt = <filename>" to specify an model specific
## prompt file to use. This will override the default and fallback prompts. This can be useful if you want a
## particular mode to focus on code or have a different style of conversation better suited to it.

## You can extend and override this file by creating a file at ~/.config/iptic-memex/models.ini
[DEFAULT]
price_unit = 1000000
interactions = completion, chat

[gpt-5]
provider = OpenAIResponses
model_name = gpt-5
# Enable streaming to exercise Responses event flow
stream = True
# Default to no storage unless user explicitly opts in via config
store = False
context_size = 256000
price_in = 1.25
price_cache_in = 0.125
price_out = 10.00
reasoning = True
## reasoning_effort can be one of: minimal, low, medium, high
reasoning_effort = medium
## verbosity can be one of: low, medium, high
verbosity = low
response_label = GPT-5

[gpt-5-mini]
provider = OpenAIResponses
model_name = gpt-5-mini
# Enable streaming to exercise Responses event flow
stream = True
# Default to no storage unless user explicitly opts in via config
store = False
context_size = 256000
reasoning = true
# reasoning_effort can be one of: low, medium, high
reasoning_effort = low
verbosity = low
price_in = 0.25
price_cache_in = 0.03
price_out = 2.00
response_label = GPT-5

[gpt-4o]
provider = OpenAI
model_name = gpt-4o
context_size = 128000
price_in = 2.5
price_out = 10
vision = True

[gpt-4o-mini]
provider = OpenAI
model_name = gpt-4o-mini
context_size = 128000
price_in = 0.15
price_out = 0.60

[gpt-4.1]
provider = OpenAI
model_name = gpt-4.1
context_size = 1047576
max_tokens = 32768
price_in = 2.00
price_out = 8.00
price_cache_in = 0.50
vision = True

[gpt-4.1-mini]
provider = OpenAI
model_name = gpt-4.1
context_size = 1047576
max_tokens = 32768
price_in = 0.40
price_out = 1.60
price_cache_in = 0.10

[gpt-4.1-nano]
provider = OpenAI
model_name = gpt-4.1
context_size = 1047576
max_tokens = 32768
price_in = 0.10
price_out = 0.40
price_cache_in = 0.03

[o1]
provider = OpenAI
model_name = o1
context_size = 200000
response_label = o1
reasoning = True
price_in = 15.00
price_out = 60.00

[o1-mini]
provider = OpenAI
model_name = o1-mini
context_size = 128000
response_label = o1-mini
reasoning = True
price_in = 1.10
price_out = 4.40

[o3-mini]
provider = OpenAI
model_name = o3-mini
context_size = 200000
response_label = o3-mini
reasoning = True
price_in = 1.10
price_out = 4.40

[haiku]
provider = Anthropic
model_name = claude-3-5-haiku-latest
context_size = 200000
max_tokens = 4096
price_in = 0.8
price_out = 4
price_cache_in = 1
price_cache_out = 0.08
vision = True

[sonnet]
provider = Anthropic
model_name = claude-sonnet-4-0
context_size = 200000
price_in = 3
price_out = 15
price_cache_in = 3.75
price_cache_out = 0.03

[opus]
provider = Anthropic
model_name = claude-opus-4-0
context_size = 200000
price_in = 15
price_out = 75
price_cache_in = 18.75
price_cache_out = 1.5

[gemini-pro]
provider = Google
model_name = gemini-1.5-pro
context_size = 2097152
max_tokens = 8192
price_in = 1.25
price_out = 5
price_cache_in = 0.3125

[gemini-flash]
provider = Google
model_name = gemini-2.0-flash-001
context_size = 1048576
max_tokens = 8192
price_in = 0.10
price_out = 0.40
vision = True

[sonar]
provider = Perplexity
model_name = sonar
context_size = 127000
price_in = 1
price_out = 1
tools = False

[sonar-pro]
provider = Perplexity
model_name = sonar-pro
context_size = 200000
price_in = 3
price_out = 15
tools = False

[sonar-reasoning]
provider = Perplexity
model_name = sonar-reasoning
context_size = 127000
price_in = 1
price_out = 5
tools = False

[sonar-reasoning-pro]
provider = Perplexity
model_name = sonar-reasoning-pro
context_size = 127000
price_in = 2
price_out = 8
tools = False

[sonar-deep-research]
provider = Perplexity
model_name = sonar-deep-research
context_size = 60000
price_in = 2
price_out = 8
tools = False

[deepseek-chat]
provider = DeepSeek
model_name = deepseek-chat
context_size = 131072
price_in = 0.14
price_out = 0.28

[deepseek-coder]
provider = DeepSeek
model_name = deepseek-coder
context_size = 131072
price_in = 0.14
price_out = 0.28

[deepseek-reasoner]
provider = DeepSeek
model_name = deepseek-reasoner
context_size = 131072
price_in = 0.14
price_out = 0.28

#[nemotron]
#provider = OpenRouter
#model_name = nvidia/nemotron-4-340b-instruct
#context_size = 4096
#response_label = Nemotron

# example of a model entry for use with llama.cpp, might be best to put these in the user's own models.ini file
#
#[llama3]
#provider = LlamaCpp
#model_name = llama3-70B
#model_path = <full path to model>
#context_size = 12000
#response_label = Llama
#extra_body = {cache_prompt:true}
#speculative = True # enable speculative decoding
#draft = 5 # number of tokens to draft, defaults to 10
#llamacpp_init = { flash_attn:true, n_threads:8 }  # optional: native llama_cpp.Llama kwargs; provider defaults can be overridden here

# example of a model entry using the managed llama.cpp server
#
#[llama-local]
#provider = LlamaCppServer
#model_name = llama-3.1-8b-instruct
#model_path = /abs/path/Meta-Llama-3.1-8B-Instruct.Q4_K_M.gguf
#draft_model_path = /abs/path/Llama-3.2-1B-Instruct-Q5_K_M.gguf  # optional: speculative decoding (-md)
#context_size = 8192
#stream = true
#tools = false
#response_label = Llama (server)
# Add model-specific extras without overriding provider defaults
#extra_flags_append = --jinja

#[gpt-oss-local]
#provider = GptOss
#binary = /abs/path/to/llama-server
#model_path = /abs/path/to/gpt-oss.gguf
#stream = true
#tools = true
#reasoning = true
#reasoning_effort = medium
#extra_flags_append = --jinja

[Mock]
provider = Mock
default = True
temperature = 0.7
max_tokens = 1000
