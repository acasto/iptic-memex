## This file contains the configuration for the models that are available in the system.
## The model names are the section names in this file. They are nicknames and not the official
## model names. To make it easier to select a model at runtime (e.g. haiku vs claude-3-haiku-20240307).
##
## Options listed here will roll-up and override the provider and default ones in config.ini, allowing you to
## have per-model settings. In addition, you can use the option "prompt = <filename>" to specify an model specific
## prompt file to use. This will override the default and fallback prompts. This can be useful if you want a
## particular mode to focus on code or have a different style of conversation better suited to it.

## You can extend and override this file by creating a file at ~/.config/iptic-memex/models.ini
[DEFAULT]
price_unit = 1000000
interactions = completion, chat

[gpt-4o-mini]
provider = OpenAI
model_name = gpt-4o-mini
context_size = 128000
price_in = 0.15
price_out = 0.60

[gpt-4-turbo]
provider = OpenAI
model_name = gpt-4-turbo
context_size = 128000
price_in = 10
price_out = 30

[gpt-4o]
provider = OpenAI
model_name = gpt-4o
context_size = 128000
price_in = 5
price_out = 15

[gpt-4]
provider = OpenAI
model_name = gpt-4
context_size = 8192
price_in = 30
price_out = 60

[haiku]
provider = Anthropic
model_name = claude-3-haiku-20240307
context_size = 200000
price_in = 0.25
price_out = 1.25

[sonnet]
provider = Anthropic
model_name = claude-3-sonnet-20240229
context_size = 200000
price_in = 3
price_out = 15

[sonnet-3.5]
provider = Anthropic
model_name = claude-3-5-sonnet-20240620
context_size = 200000
price_in = 3
price_out = 15

[opus]
provider = Anthropic
model_name = claude-3-opus-20240229
context_size = 200000
price_in = 15
price_out = 75

[gemini-pro]
provider = Google
model_name = gemini-1.5-pro
context_size = 1000000
price_in = 3.5
price_out = 10.5

[gemini-flash]
provider = Google
model_name = gemini-1.5-flash
context_size = 1000000
price_in = 0.35
price_out = 1.05

[gemini-pro-1.0]
provider = Google
model_name = gemini-1.0-pro
context_size = 1000000
price_in = 0.5
price_out = 1.5

[ppl-small-chat]
provider = Perplexity
model_name = llama-3-sonar-small-32k-chat
context_size = 32768
price_in = 0.20
price_out = 0.20

[ppl-small-online]
provider = Perplexity
model_name = llama-3-sonar-small-32k-online
context_size = 32768
price_in = 0.20
price_out = 0.20

[ppl-large-chat] 
provider = Perplexity
model_name = llama-3-sonar-large-32k-chat
context_size = 32768
price_in = 1.00
price_out = 1.00

[ppl-large-online] 
provider = Perplexity 
model_name = llama-3-sonar-large-32k-online 
context_size = 32768 
price_in = 1.00
price_out = 1.00

[groq-llama-8b]
provider = Groq
model_name = llama3-8b-8192
context_size = 8192
price_in = 0.00
price_out = 0.00

[groq-llama-70b]
provider = Groq
model_name = llama3-70b-8192
context_size = 8192
price_in = 0.00
price_out = 0.00

[codestral]
provider = Codestral
model_name = codestral-latest
context_size = 32768
price_in = 0.00
price_out = 0.00

[mistral-7b]
provider = Mistral
model_name = open-mistral-7b
context_size = 32768
price_in = 0.00
price_out = 0.00

[mixtral-8x7b]
provider = Mistral
model_name = open-mixtral-8x7b
context_size = 32768
price_in = 0.00
price_out = 0.00

[mixtral-8x22b]
provider = Mistral
model_name = open-mixtral-8x22b
context_size = 65536
price_in = 0.00
price_out = 0.00

[mistral-nemo]
provider = Mistral
model_name = open-mistral-nemo
context_size = 131072
price_in = 0.00
price_out = 0.00

[deepseek-chat]
provider = DeepSeek
model_name = deepseek-chat
context_size = 131072
price_in = 0.14
price_out = 0.28

[deepseek-coder]
provider = DeepSeek
model_name = deepseek-coder
context_size = 131072
price_in = 0.14
price_out = 0.28

[command-r]
provider = Cohere
model_name = command-r
context_size = 131072
price_in = 0.50
price_out = 1.50

[command-r-plus]
provider = Cohere
model_name = command-r-plus
context_size = 131072
price_in = 3.00
price_out = 15.00

[fw-llama3.1-405b]
provider = Fireworks
model_name = accounts/fireworks/models/llama-v3p1-405b-instruct
context_size = 131072
price_in = 3.00
price_out = 3.00

#[nemotron]
#provider = OpenRouter
#model_name = nvidia/nemotron-4-340b-instruct
#context_size = 4096
#response_label = "> Nemotron: "

# example of a model entry for use with llama.cpp, might be best to put these in the user's own models.ini file
#
#[llama3]
#provider = LlamaCpp
#model_name = llama3-70B
#context_size = 12000
#response_label = "> Llama: "
#extra_body = {cache_prompt:true}
