[DEFAULT]
prompt_directory = prompts
chats_directory = sessions
chat_format = md
user_config = ~/.config/iptic-memex/config.ini
user_models = ~/.config/iptic-memex/models.ini
user_prompts = ~/prompts
#user_actions = ~/.config/iptic-memex/actions
user_db = ~/.config/iptic-memex/db.sqlite
vector_db = ~/.config/iptic-memex/vector_store
template_handler = prompt_template, prompt_template_memory
enable_tools = True
tools = True
vision = False
session_budget = 5.00
default_model = gpt-4o
#temperature = 0.7
max_tokens = 8192
stream = False
stream_delay = 0.1
stream_buffer = 50
# output_filters can be a comma separated list of filter actions, e.g., think_tag, tool_call
# output_filters = think_tag,tool_call
# Optional placeholders (default blank if unset):
# think_placeholder = ⟦hidden:think⟧
# tool_placeholder  = ⟦hidden:{name}⟧
highlighting = True
colors = True
# spinner_style = dots, line, block, arrow, pulse, blocks, none
spinner_style = block
#output_level = DEBUG, INFO, WARNING, ERROR, CRITICAL
output_level = INFO
output_styles = {"DEBUG": {"fg": "gray", "dim": true}, "ERROR": {"fg": "bright_red", "bold": true}, "status": {"fg": "#00A5FF"}, "success": {"fg": "bright_green", "bold": true}}
user_label = "> User: "
user_label_color = gray
response_label = "> AI: "
response_label_color = green
# context_sent controls the number of messages passed to the provider. Values are: all, none, last_<n>, or first_<n>.
# Note: 'none' behaves like 'last_1' since it would make no sense to send no messages.
context_sent = all
show_context_summary = True

[PROMPTS]
;default = default.txt, tools, memories
default = default.txt
pseudo_tools = tool_general, tool_cmd_local, tool_rag, tool_websearch, tool_file, tool_memory, tool_openlink, tool_youtrack

[AGENT]
default_model = gpt-5
# Defaults for Agent Steps Mode
default_steps = 1
writes_policy = deny
status_tags = True
show_context_details = True
context_detail_max_chars = 4000
output = final
docker_always_ephemeral = True

[WEB]
open_browser = 1
#host = 127.0.0.1
#port = 8765


[TOOLS]
active_tools = file,cmd,memory,openlink,ragsearch,websearch,youtrack
inactive_tools = math
#active_tools_agent =
#inactive_tools_agent = math,youtrack
#cmd_tool = assistant_cmd_tool     # Default handler
#docker_env = ephemeral
#websearch_tool = assistant_websearch_tool    # Default handler
#sonar_citations = True
allow_auto_submit = True
write_confirm = True
show_diff_with_confirm = True
confirm_large_input = True
large_input_limit = 4000
timeout = 15
base_directory = working
ensure_trailing_newline = True
# Tools mode settings
tool_mode = official
pseudo_tool_prompt = pseudo_tools
#edit_model = gpt-4.1-mini
#edit_confirm = True
#embedding_model = text-embedding-3-small
#embedding_provider = openai
#summary_model = llama-3b
#summary_prompt = summary
#vision_model = gemini-flash-thinking
#vision_prompt = image_summary
#search_prompt = websearch
#search_model= sonar
#rag_top_k = 8
#rag_per_index_cap = None
#rag_preview_lines = 3
#rag_similarity_threshold = 0.0
#rag_attach_mode = summary
#rag_total_chars_budget = 20000
#rag_group_by_file = True
#rag_merge_adjacent = True
#rag_merge_gap = 5
#rag_max_file_mb = 10
# Optional extension allowlist for RAG discovery (comma-separated):
#rag_included_exts = .md,.mdx,.txt,.rst,.pdf,.docx,.xlsx
#
# Optional global discovery filters (comma-separated glob patterns; matched relative to each index root):
#rag_default_include = **/*.md, **/*.mdx, **/*.txt, **/*.rst
#rag_default_exclude = .git, node_modules, __pycache__, .venv, **/*.png, **/*.jpg
# Optional discovery size limit (MB) for individual files (defaults to 10):


#[RAG]
#notes = path to notes directory, e.g., ~/notes
# Note: extension allowlist lives under [TOOLS] via rag_included_exts.
#notes_exclude = .git, node_modules, __pycache__, .venv, **/*.png, **/*.jpg

## Docker environments, selected in [TOOLS] with docker_env
#
#[EPHEMERAL]
#docker_image = sandbox-assistant:latest
#docker_run_options = --network bridge --memory 512m --cpus=4
# to add mountpoints to the docker container use: -v <host_path>:<container_path><(:ro|rw)
#persistent = false

#[WEBDEV]
#docker_image = django-dev-image
#docker_run_options = --network bridge --memory 512m --cpus=4 -p 8000:8000
#persistent = true      # Explicitly declare this is persistent
#docker_name = django-dev  # Name for the persistent container

## Providers
## Note: Model specific settings are now in models.ini This file is for API keys and other provider level settings.
##       By default all models associated with a provider will be available, but if you wish to restrict access to
##       specific models you can do so by setting the models key to a comma separated list of model names.
##       (Model names are the section names in models.ini, not the full model names from the provider docs)
##
## At minimum you need to set a provider below to active and provide an API key either here or through the environment.

[LlamaCpp]
#active = True
n_gpu_layers = -1
# Default to pseudo-tools for llama.cpp; can be 'official' or 'none' if needed
tool_mode = pseudo
# Optional: native llama_cpp.Llama kwargs applied to all LlamaCpp models (overridable per-model via models.ini)
#llamacpp_init = { flash_attn:true, n_threads:8 }

[LlamaCppServer]
# Path to llama.cpp OpenAI-compatible server binary (examples/server)
#binary = /abs/path/to/llama-server
host = 127.0.0.1
port_range = 40100-40149
startup_timeout = 90
use_api_key = true
# Prefer pseudo tools; llama-server does not support official function tools
tool_mode = pseudo
# Logging (disabled by default). Provide either log_path or log_dir to enable.
#log = false
#log_path = ~/.cache/iptic-memex/llama-server.log
#log_dir = ~/.cache/iptic-memex
# Extra flags to pass through (space-delimited, shlex-split)
# For example, disable mmap globally and allow models to append their own flags.
#extra_flags = --no-mmap
# Optional tuning passed to the server:
#threads = 8
#n_gpu_layers = -1
#context_size = 8192
#cont_batching = true
#parallel = 2
#alias = llama-local
#chat_template = llama3
#extra_server_args = --metrics
extra_flags_append = --flash-attn

[GptOss]
harmony_knowledge_cutoff = 2024-06
include_browser_tool = false
include_python_tool = false
#extra_flags_append = --reasoning-format none

[OpenAI]
#active = True
#api_key =
#organization =
#project =
#models = gpt-3.5-turbo, gpt-4, gpt-4o
endpoint = https://api.openai.com/v1/chat/completions
response_label = "> ChatGPT: "
tokenizer = tiktoken
max_completion_tokens = 50000
# Reasoning/verbosity options (for reasoning-capable models like o-series, 4o, GPT‑5)
# Set reasoning=true to enable reasoning features. When enabled:
#   - The provider sends extra_body.max_completion_tokens (and drops top-level max_tokens).
#   - reasoning_effort is forwarded (supports minimal|low|medium|high; values are lowercased).
#   - verbosity is forwarded (low|medium|high) to steer short vs long answers.
# Pricing/usage:
#   - bill_reasoning_as_output (default True) controls whether reasoning tokens are counted as output for cost calc.
# Streaming:
#   - stream_options (default True) controls whether stream_options={include_usage: true} is sent for streaming usage stats.
# Example:
# reasoning = true
# reasoning_effort = minimal
# verbosity = low
# bill_reasoning_as_output = True
# stream_options = True

[Anthropic]
#active = False
#api_key =
endpoint = https://api.anthropic.com/v1/messages
response_label = "> Claude: "
prompt_caching = True

[Google]
#active = False
#api_key =
endpoint = https://api.google.com/v1/chat/completions
response_label = "> Gemini: "
prompt_caching = False
cache_ttl = 5
# Safety settings (BLOCK_NONE, BLOCK_ONLY_HIGH, BLOCK_MEDIUM_AND_ABOVE, BLOCK_LOW_AND_ABOVE)
safety_default = BLOCK_NONE
# Override specific categories
# safety_harassment = BLOCK_LOW_AND_ABOVE
# safety_hate_speech = BLOCK_LOW_AND_ABOVE
# safety_sexually_explicit = BLOCK_LOW_AND_ABOVE
# safety_dangerous_content = BLOCK_LOW_AND_ABOVE

[Google-OAI]
alias = OpenAI
#api_key = 
base_url = https://generativelanguage.googleapis.com/v1beta/openai/

[OpenRouter]
alias = OpenAI
#active = False
#api_key =
base_url = https://openrouter.ai/api/v1
response_label = "> OpenRouter: "

[Perplexity]
alias = OpenAI
#active False
#api_key = 
base_url = https://api.perplexity.ai
response_label = "> Perplexity: "
use_old_system_role = True

[Groq]
alias = OpenAI
#active = False
#api_key =
base_url = https://api.groq.com/openai/v1
response_label = "> Groq: "
stream_options = False
use_old_system_role = True
use_simple_message_format = True

[Codestral]
alias = OpenAI
#active = False
#api_key = 
base_url = https://codestral.mistral.ai/v1
response_label = "> Codestral: "
# while mostly OpenAI compatible the stream_options parameter seems to break the mistral response
stream_options = False

[Mistral]
alias = OpenAI
#active = False
#api_key = 
base_url = https://api.mistral.ai/v1
response_label = "> Mistral: "
# while mostly OpenAI compatible the stream_options parameter seems to break the mistral response
stream_options = False

[DeepSeek]
alias = OpenAI
#active = False
#api_key = 
base_url = https://api.deepseek.com
response_label = "> DeepSeek: "
use_old_system_role = True
use_simple_message_format = True

[Fireworks]
alias = OpenAI
#active = False
#api_key = 
base_url = https://api.fireworks.ai/inference/v1
stream_options = False
use_old_system_role = True

[Together]
alias = OpenAI
#active = False
#api_key = 
base_url = https://api.together.xyz/v1

[Cohere]
alias = OpenAI
#active = False
#api_key =
response_label = "> Cohere: "
base_url = https://api.cohere.ai/compatibility/v1
max_tokens = 4096

[Localhost]
alias = OpenAI
#active = False
#api_key =
base_url = http://127.0.0.1:8080
endpoint = /v1/chat/completions
response_label = "> Assistant: "
timeout = 1200

[Brave]
type = search
#api_key =
endpoint = https://api.search.brave.com/res/v1/web/search
summary_endpoint = https://api.search.brave.com/res/v1/summarizer/search

[YOUTRACK]
#base_url = https://your-instance.myjetbrains.com/
#api_key = YOUR_PERMANENT_TOKEN_HERE
# Default filter for getting issues (uses YouTrack query syntax)
default_state_filter = status:{Open} or status:{In Progress}
# Field name mappings (customize based on your YouTrack configuration)
state_field_name = Status
priority_field_name = Priority  
type_field_name = Type
assignee_field_name = Assignee
# Timezone setting (e.g., 'America/New_York')
timezone = UTC

[OpenAIResponses]
active = True
#api_key =
# For OpenAI Responses API, the SDK defaults work (base_url=https://api.openai.com/v1).
# Only set base_url if using a compatible non-OpenAI endpoint.
# base_url = https://api.openai.com/v1
# endpoint = /responses
response_label = "> OpenAI (Responses): "
# Streaming usage stats are supported; leave enabled by default.
stream_options = True
# Privacy defaults: keep storage off unless explicitly enabled by users.
store = False
use_previous_response = False
# Tool calling defaults: off unless enabled by model/provider choice as usual.
tool_mode = official
; When store/use_previous_response are enabled, minimize input by only
; sending the latest window (e.g., function_call + function_call_output or
; the last user message) instead of the full history.
chain_minimize_input = True
nullable_optionals = True
