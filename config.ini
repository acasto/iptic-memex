[DEFAULT]
prompt_directory = prompts
chats_directory = sessions
chat_format = md
user_config = ~/.config/iptic-memex/config.ini
user_models = ~/.config/iptic-memex/models.ini
user_prompts = ~/prompts
#user_actions = ~/.config/iptic-memex/actions
user_db = ~/.config/iptic-memex/db.sqlite
template_handler = prompt_template, prompt_template_chat, prompt_template_memory
enable_tools = True
vision = False
session_budget = 5.00
default_model = gpt-4o
#temperature = 0.7
max_tokens = 8192
stream = False
stream_delay = 0.1
stream_buffer = 50
# output_filters can be a comma separated list of filter actions, e.g., think_tag, tool_call
# output_filters = think_tag,tool_call
# Optional placeholders (default blank if unset):
# think_placeholder = ⟦hidden:think⟧
# tool_placeholder  = ⟦hidden:{name}⟧
highlighting = True
colors = True
# spinner_style = dots, line, block, arrow, pulse, blocks, none
spinner_style = block
#output_level = DEBUG, INFO, WARNING, ERROR, CRITICAL
output_level = INFO
output_styles = {"DEBUG": {"fg": "gray", "dim": true}, "ERROR": {"fg": "bright_red", "bold": true}, "status": {"fg": "#00A5FF"}, "success": {"fg": "bright_green", "bold": true}}
user_label = User
user_label_color = gray
response_label = AI
response_label_color = green
# context_sent controls the number of messages passed to the provider. Values are: all, none, last_<n>, or first_<n>.
# Note: 'none' behaves like 'last_1' since it would make no sense to send no messages.
context_sent = all
show_context_summary = True

[PROMPTS]
;default = default.txt, tools, memories
default = default.txt
pseudo_tools = tool_general, tool_cmd_docker, tool_file, tool_rag, tool_websearch, tool_memory, tool_openlink, tool_youtrack, tool_persona_review

[AGENT]
default_model = gpt-5
# Defaults for Agent Steps Mode
default_steps = 1
writes_policy = deny
status_tags = True
show_context_details = True
context_detail_max_chars = 4000
output = final
docker_always_ephemeral = True
# Optional input gating for non-interactive runs (agent/completion). If unset, no gate is applied.
#large_input_limit = 32000
;
; Non-interactive (Agent/Completion) tool profile
; If unset, falls back to [TOOLS].active_tools. Use CLI --tools to override per run.
active_tools =
; Hard blocklist for non-interactive runs; always applied even with --tools.
blocked_tools =
; Gate MCP in non-interactive runs (Agent/Completion). Interactive modes use [MCP].active as usual.
use_mcp = false
; Optional slim MCP server list (CSV) for non-interactive runs. If unset, uses [MCP].mcp_servers or all defined servers.
available_mcp =

[WEB]
open_browser = 1
#host = 127.0.0.1
#port = 8765

[TUI]
status_max_lines = 200

[LOG]
# Centralized logging controls (disabled by default)
active = False
dir = logs
per_run = True
file = assistant.log
format = json
mirror_to_console = False
symlink_latest = True
redact = True
redact_keys = api_key,authorization,token,password,secret,key
truncate_chars = 2000

# Aspect toggles
## log_settings = off|basic|detail (default basic)
log_settings = basic
## log_messages = off|basic|detail|trace (default off)
log_messages = off
## log_tool_use = off|basic|detail|trace (default basic)
log_tool_use = basic
## log_cmd = off|minimal|detail|trace (default minimal)
log_cmd = minimal
## log_provider = off|basic|detail (default basic)
log_provider = basic
## log_mcp = off|basic|detail (default off)
log_mcp = off
## log_rag = off|basic|detail (default off)
log_rag = off
## log_actions = off|basic|detail (default off)
log_actions = off
## log_errors = off|basic|trace (default basic)
log_errors = basic
## log_usage = off|basic (default basic)
log_usage = basic
## log_web = off|basic|detail (default off)
log_web = off
## log_tui = off|basic|detail (default off)
log_tui = off

[TOOLS]
active_tools = cmd,file,memory,openlink,ragsearch,websearch,youtrack,persona_review
inactive_tools = math
#cmd_tool = assistant_cmd_tool     # Default handler
#docker_env = ephemeral
#websearch_tool = assistant_websearch_tool    # Default handler
#sonar_citations = True
allow_auto_submit = True
auto_submit_max_turns = 15
write_confirm = True
show_diff_with_confirm = True
confirm_large_input = True
large_input_limit = 4000
timeout = 15
base_directory = working
ensure_trailing_newline = True
# Tools mode settings
tool_mode = official
pseudo_tool_prompt = pseudo_tools
#edit_model = gpt-5-mini
#edit_confirm = True
#embedding_model = text-embedding-3-small
#embedding_provider = openai
#summary_model = gpt-5-mini
#summary_prompt = summary
#vision_model = gemini-flash-thinking
#vision_prompt = image_summary
#search_prompt = websearch
#search_model= sonar

[MCP]
# Global feature flag for Model Context Protocol integration.
# When false (default), MCP actions/tools are hidden and inactive.
active = False
;use_sdk = True
;connect_timeout = 10s
;call_timeout = 20s
;retries = 0
; Optional delay to give managed servers a moment to initialize before discovery (non-interactive runs only)
;startup_delay = 0.5s
; Opt-in generic HTTP JSON fallback (/tools, /call, /resource). Prefer SDK.
;http_fallback = false
;max_resource_bytes = 1048576
;allowed_resource_schemes = http,https,doc
# allow_dynamic_tools = True
;mcp_servers = context7
;autoload = context7
; Control alias creation for app-side registration (pretty short names)
;auto_alias = true

;[MCP.context7]
; transport = provider|http|stdio
;transport = http
;url = https://mcp.context7.com/mcp
;headers = {"CONTEXT7_API_KEY": "${env:CONTEXT7_API_KEY}"}
; optional provider hints:
;allowed_tools = tool1,tool2
;require_approval = never
; Per-server behavior overrides:
;autoload = true
;auto_alias = true

[RAG]
active = False
vector_db = ~/.config/iptic-memex/vector_store
# Declare indexes and defaults. Example:
#indexes = notes, docs
#included_exts = .md,.mdx,.txt,.rst,.pdf,.docx,.xlsx
#default_include = **/*.md, **/*.mdx, **/*.txt, **/*.rst
#default_exclude = .git, node_modules, __pycache__, .venv, **/*.png, **/*.jpg
#max_file_mb = 10
# Global tuning knobs (optional):
#top_k = 8
#per_index_cap =
#preview_lines = 3
#similarity_threshold = 0.0
#attach_mode = summary
#total_chars_budget = 20000
#group_by_file = True
#merge_adjacent = True
#merge_gap = 5

# Per-index sections:
#[RAG.notes]
#path = ~/notes
#include = **/*.md
#exclude = .git, node_modules, __pycache__, .venv, **/*.png, **/*.jpg

#[RAG.docs]
#path = ~/docs

## Docker environments, selected in [TOOLS] with docker_env
#
[EPHEMERAL]
#docker_image = sandbox-assistant:latest
#docker_run_options = --network bridge --memory 512m --cpus=4
# to add mountpoints to the docker container use: -v <host_path>:<container_path><(:ro|rw)
persistent = false
tmp_mount = bind
tmp_value = .assistant-tmp
set_tmpdir_env = true

[WEBDEV]
#docker_image = django-dev-image
#docker_run_options = --network bridge --memory 512m --cpus=4 -p 8000:8000
#persistent = true      # Explicitly declare this is persistent
#docker_name = django-dev  # Name for the persistent container

## Providers
## Note: Model specific settings are now in models.ini This file is for API keys and other provider level settings.
##       By default all models associated with a provider will be available, but if you wish to restrict access to
##       specific models you can do so by setting the models key to a comma separated list of model names.
##       (Model names are the section names in models.ini, not the full model names from the provider docs)
##
## At minimum you need to set a provider below to active and provide an API key either here or through the environment.

[LlamaCpp]
#active = True
n_gpu_layers = -1
# Default to pseudo-tools for llama.cpp; can be 'official' or 'none' if needed
tool_mode = pseudo
# Optional: native llama_cpp.Llama kwargs applied to all LlamaCpp models (overridable per-model via models.ini)
#llamacpp_init = { flash_attn:true, n_threads:8 }
#output_filters = think_tag,tool_call


[LlamaCppServer]
# Path to llama.cpp OpenAI-compatible server binary (examples/server)
#binary = /abs/path/to/llama-server
host = 127.0.0.1
port_range = 40100-40149
startup_timeout = 90
use_api_key = true
# Prefer pseudo tools; llama-server does not support official function tools
tool_mode = pseudo
# Logging (disabled by default). Provide either log_path or log_dir to enable.
#log = false
#log_path = ~/.cache/iptic-memex/llama-server.log
#log_dir = ~/.cache/iptic-memex
# Extra flags to pass through (space-delimited, shlex-split)
# For example, disable mmap globally and allow models to append their own flags.
#extra_flags = --no-mmap
# Optional tuning passed to the server:
#threads = 8
#n_gpu_layers = -1
#context_size = 8192
#cont_batching = true
#parallel = 2
#alias = llama-local
#chat_template = llama3
#extra_server_args = --metrics
extra_flags_append = --flash-attn on
#output_filters = think_tag,tool_call

[Mlx]
active = False
tool_mode = pseudo

[GptOss]
harmony_knowledge_cutoff = 2024-06
include_browser_tool = false
include_python_tool = false
#extra_flags_append = --reasoning-format none

## OpenAI Responses API provider
[OpenAIResponses]
active = True
#api_key =
# For OpenAI Responses API, the SDK defaults work (base_url=https://api.openai.com/v1).
# Only set base_url if using a compatible non-OpenAI endpoint.
# base_url = https://api.openai.com/v1
# endpoint = /responses
response_label = OpenAI (Responses)
# Streaming usage stats are supported; leave enabled by default.
stream_options = True
# Privacy defaults: keep storage off unless explicitly enabled by users.
store = False
use_previous_response = False
# Tool calling defaults: off unless enabled by model/provider choice as usual.
tool_mode = official
; When store/use_previous_response are enabled, the provider automatically
; minimizes input by only sending the newest window (e.g., function_call +
; function_call_output or the latest user turn) instead of resending history.
nullable_optionals = True
; MCP provider pass-through configuration (for providers that support it).
; Pass-through is gated by [MCP].active (no provider-local toggle).
; List servers as label=url pairs below; optional per-server settings follow.
;mcp_servers = math=https://mcp.example.com/math, files=https://mcp.example.com/files
;mcp_headers_math = {"Authorization": "Bearer ..."}
;mcp_allowed_math = add,sub,mul,div
;mcp_require_approval = never   ; or always; can also set mcp_require_approval_<label>

## OpenAI chat completions API provider and compatible endpoints
[OpenAI]
#active = True
#api_key =
#organization =
#project =
#models = gpt-3.5-turbo, gpt-4, gpt-4o
endpoint = https://api.openai.com/v1/chat/completions
response_label = ChatGPT
tokenizer = tiktoken
max_completion_tokens = 50000
# Reasoning/verbosity options (for reasoning-capable models like o-series, 4o, GPT‑5)
# Set reasoning=true to enable reasoning features. When enabled:
#   - The provider sends extra_body.max_completion_tokens (and drops top-level max_tokens).
#   - reasoning_effort is forwarded (supports minimal|low|medium|high; values are lowercased).
#   - verbosity is forwarded (low|medium|high) to steer short vs long answers.
# Pricing/usage:
#   - bill_reasoning_as_output (default True) controls whether reasoning tokens are counted as output for cost calc.
# Streaming:
#   - stream_options (default True) controls whether stream_options={include_usage: true} is sent for streaming usage stats.
# Example:
# reasoning = true
# reasoning_effort = minimal
# verbosity = low
# bill_reasoning_as_output = True
# stream_options = True

[Anthropic]
#active = False
#api_key =
endpoint = https://api.anthropic.com/v1/messages
response_label = Claude
prompt_caching = True

[Google]
#active = False
#api_key =
endpoint = https://api.google.com/v1/chat/completions
response_label = Gemini
prompt_caching = False
cache_ttl = 5
# Safety settings (BLOCK_NONE, BLOCK_ONLY_HIGH, BLOCK_MEDIUM_AND_ABOVE, BLOCK_LOW_AND_ABOVE)
safety_default = BLOCK_NONE
# Override specific categories
# safety_harassment = BLOCK_LOW_AND_ABOVE
# safety_hate_speech = BLOCK_LOW_AND_ABOVE
# safety_sexually_explicit = BLOCK_LOW_AND_ABOVE
# safety_dangerous_content = BLOCK_LOW_AND_ABOVE

[Google-OAI]
alias = OpenAI
#api_key = 
base_url = https://generativelanguage.googleapis.com/v1beta/openai/

## Subclasses the OpenAI provider to support the Kimi K2 Thinking reasoning traces
[Moonshot]
#active = False
# api_key =
base_url = https://api.moonshot.ai/v1
use_old_system_role = True

[OpenRouter]
alias = OpenAI
#active = False
#api_key =
base_url = https://openrouter.ai/api/v1
response_label = OpenRouter

[Perplexity]
alias = OpenAI
#active False
#api_key = 
base_url = https://api.perplexity.ai
response_label = Perplexity
use_old_system_role = True

[Groq]
alias = OpenAI
#active = False
#api_key =
base_url = https://api.groq.com/openai/v1
response_label = Groq
stream_options = False
use_old_system_role = True
use_simple_message_format = True

[Codestral]
alias = OpenAI
#active = False
#api_key = 
base_url = https://codestral.mistral.ai/v1
response_label = Codestral
# while mostly OpenAI compatible the stream_options parameter seems to break the mistral response
stream_options = False

[Mistral]
alias = OpenAI
#active = False
#api_key = 
base_url = https://api.mistral.ai/v1
response_label = Mistral
# while mostly OpenAI compatible the stream_options parameter seems to break the mistral response
stream_options = False

[DeepSeek]
alias = OpenAI
#active = False
#api_key = 
base_url = https://api.deepseek.com
response_label = DeepSeek
use_old_system_role = True
use_simple_message_format = True

[Fireworks]
alias = OpenAI
#active = False
#api_key = 
base_url = https://api.fireworks.ai/inference/v1
stream_options = False
use_old_system_role = True

[Together]
alias = OpenAI
#active = False
#api_key = 
base_url = https://api.together.xyz/v1

[Cohere]
alias = OpenAI
#active = False
#api_key =
response_label = Cohere
base_url = https://api.cohere.ai/compatibility/v1
max_tokens = 4096

[Localhost]
alias = OpenAI
#active = False
#api_key =
base_url = http://127.0.0.1:8080
endpoint = /v1/chat/completions
response_label = Assistant
timeout = 1200

[Brave]
type = search
#api_key =
endpoint = https://api.search.brave.com/res/v1/web/search
summary_endpoint = https://api.search.brave.com/res/v1/summarizer/search

[YOUTRACK]
#base_url = https://your-instance.myjetbrains.com/
#api_key = YOUR_PERMANENT_TOKEN_HERE
# Default filter for getting issues (uses YouTrack query syntax)
default_state_filter = status:{Open} or status:{In Progress}
# Field name mappings (customize based on your YouTrack configuration)
state_field_name = Status
priority_field_name = Priority  
type_field_name = Type
assignee_field_name = Assignee
# Timezone setting (e.g., 'America/New_York')
timezone = UTC
