[DEFAULT]
prompt_directory = prompts
chats_directory = sessions
chat_format = md
fallback_prompt = You are a helpful assistant that can answer questions.
user_config = ~/.config/iptic-memex/config.ini
user_models = ~/.config/iptic-memex/models.ini
#user_actions = ~/.config/iptic-memex/actions
user_db = ~/.config/iptic-memex/db.sqlite
template_handler = default
default_model = gpt-4o
#temperature = 0.7
max_tokens = 2048
stream = False
stream_delay = 0.1
highlighting = True
colors = True
# spinner_style = dots, line, block, arrow, pulse, blocks, none
spinner_style = block
#output_level = INFO, DEBUG, INFO, WARNING, ERROR, CRITICAL
output_level = INFO
output_styles = {"DEBUG": {"fg": "gray", "dim": true}, "ERROR": {"fg": "bright_red", "bold": true}, "status": {"fg": "#00A5FF"}, "success": {"fg": "bright_green", "bold": true}}
user_label = "> User: "
user_label_color = gray
response_label = "> AI: "
response_label_color = green
# context_sent controls the number of messages passed to the provider. Values are: all, none, last_<n>, or first_<n>.
# Note: 'none' behaves like 'last_1' since it would make no sense to send no messages.
context_sent = all

[PROMPTS]
default = default.txt
#bob = test1, test2

[TOOLS]
allow_auto_submit = True
write_confirm = True
max_input = 4000
base_directory = working
#summary_model = llama-3b
#summary_prompt = summary

## Providers
## Note: Model specific settings are now in models.ini This file is for API keys and other provider level settings.
##       By default all models associated with a provider will be available, but if you wish to restrict access to
##       specific models you can do so by setting the models key to a comma separated list of model names.
##       (Model names are the section names in models.ini, not the full model names from the provider docs)
##
## At minimum you need to set a provider below to active and provide an API key either here or through the environment.

[LlamaCpp]
#active = True
n_gpu_layers = -1

[OpenAI]
#active = True
#api_key =
#organization =
#project =
#models = gpt-3.5-turbo, gpt-4, gpt-4o
endpoint = https://api.openai.com/v1/chat/completions
response_label = "> ChatGPT: "
tokenizer = tiktoken

[Anthropic]
#active = False
#api_key =
endpoint = https://api.anthropic.com/v1/messages
response_label = "> Claude: "

[Google]
#active = False
#api_key =
endpoint = https://api.google.com/v1/chat/completions
response_label = "> Gemini: "

[OpenRouter]
alias = OpenAI
#active = False
#api_key =
base_url = https://openrouter.ai/api/v1
response_label = "> OpenRouter: "

[Perplexity]
alias = OpenAI
#active False
#api_key = 
base_url = https://api.perplexity.ai
response_label = "> Perplexity: "

[Groq]
alias = OpenAI
#active = False
#api_key =
base_url = https://api.groq.com/openai/v1
response_label = "> Groq: "
stream_options = False

[Codestral]
alias = OpenAI
#active = False
#api_key = 
base_url = https://codestral.mistral.ai/v1
response_label = "> Codestral: "
# while mostly OpenAI compatible the stream_options parameter seems to break the mistral response
stream_options = False

[Mistral]
alias = OpenAI
#active = False
#api_key = 
base_url = https://api.mistral.ai/v1
response_label = "> Mistral: "
# while mostly OpenAI compatible the stream_options parameter seems to break the mistral response
stream_options = False

[DeepSeek]
alias = OpenAI
#active = False
#api_key = 
base_url = https://api.deepseek.com
response_label = "> DeepSeek: "

[Fireworks]
alias = OpenAI
#active = False
#api_key = 
base_url = https://api.fireworks.ai/inference/v1
stream_options = False

[Together]
alias = OpenAI
#active = False
#api_key = 
base_url = https://api.together.xyz/v1

[Cohere]
#active = False
#api_key =
response_label = "> Cohere: "

[Localhost]
alias = OpenAI
#active = False
#api_key =
base_url = http://127.0.0.1:8080
endpoint = /v1/chat/completions
response_label = "> Assistant: "
timeout = 1200

[Brave]
type = search
#api_key =
endpoint = https://api.search.brave.com/res/v1/web/search
summary_endpoint = https://api.search.brave.com/res/v1/summarizer/search

